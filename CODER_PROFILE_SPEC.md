# Coder Profile - åŠŸèƒ½è§„æ ¼è¯´æ˜

## ç›®æ ‡ç”¨æˆ·

ä½¿ç”¨ Coding Agent å·¥å…·çš„ç¨‹åºå‘˜ï¼š
- **Claude Code** ç”¨æˆ·ï¼ˆAnthropic SDKï¼‰
- **GitHub Copilot / Codex CLI** ç”¨æˆ·ï¼ˆOpenAI SDKï¼‰
- **Cursor** ç”¨æˆ·ï¼ˆOpenAI-compatibleï¼‰
- **Continue.dev** ç”¨æˆ·
- **Aider** ç”¨æˆ·
- ä»»ä½•éœ€è¦ IDE çº§åˆ« LLM é›†æˆçš„å¼€å‘è€…

## æ ¸å¿ƒç—›ç‚¹

### ç—›ç‚¹1: åè®®ä¸å…¼å®¹ â­â­â­
**é—®é¢˜**:
- æˆ‘ç”¨ Claude Codeï¼Œä½†æƒ³è°ƒç”¨ OpenAI çš„ GPT-4
- æˆ‘ç”¨ Cursorï¼ˆOpenAIï¼‰ï¼Œä½†æƒ³è°ƒç”¨ Anthropic çš„ Claude
- å·¥å…·é”å®šåœ¨ä¸€ä¸ªåè®®ä¸Šï¼Œæ— æ³•åˆ‡æ¢ provider

**è§£å†³æ–¹æ¡ˆ**: **åŒå‘åè®®ç¿»è¯‘**
- OpenAI API â†’ Anthropic Messages API
- Anthropic API â†’ OpenAI Chat Completions API
- å®Œå…¨é€æ˜ï¼Œé›¶é…ç½®

### ç—›ç‚¹2: å·¥å…·è°ƒç”¨æ— é™å¾ªç¯ â­â­â­
**é—®é¢˜**:
- Codex CLI è°ƒç”¨ Claude æ—¶ï¼Œtool calling é‡å¤è§¦å‘
- Agent é™·å…¥æ— é™å¾ªç¯ï¼ˆåŒä¸€ä¸ª tool è¢«åå¤è°ƒç”¨ï¼‰
- æµªè´¹ tokenï¼Œæµªè´¹é’±ï¼Œæµªè´¹æ—¶é—´

**è§£å†³æ–¹æ¡ˆ**: **æ™ºèƒ½å»é‡æ£€æµ‹**
- æ£€æµ‹é‡å¤çš„ tool callï¼ˆç›¸åŒ name + argumentsï¼‰
- è‡ªåŠ¨æ‰“æ–­æ— é™å¾ªç¯
- å¯é…ç½®å»é‡ç­–ç•¥

### ç—›ç‚¹3: é•¿æ—¶é—´ä¼šè¯è¶…æ—¶ â­â­
**é—®é¢˜**:
- Coding session å¯èƒ½æŒç»­å‡ å°æ—¶
- é»˜è®¤è¶…æ—¶ï¼ˆ30s-60sï¼‰å¤ªçŸ­
- å¤æ‚ä»£ç ç”Ÿæˆéœ€è¦æ›´é•¿æ—¶é—´

**è§£å†³æ–¹æ¡ˆ**: **é•¿è¶…æ—¶é…ç½®**
- é»˜è®¤ 5 åˆ†é’Ÿè¶…æ—¶
- æ”¯æŒ streaming ä¿æŒè¿æ¥
- å¯é…ç½®è¶…æ—¶ç­–ç•¥

### ç—›ç‚¹4: æˆæœ¬å¤±æ§ â­â­
**é—®é¢˜**:
- GPT-4 å¾ˆè´µï¼Œä½†æœ‰æ—¶å€™ GPT-3.5 å°±å¤Ÿäº†
- ä¸çŸ¥é“å“ªä¸ª task èŠ±äº†å¤šå°‘é’±
- æœˆåº•è´¦å•çˆ†ç‚¸

**è§£å†³æ–¹æ¡ˆ**: **æ™ºèƒ½è·¯ç”± + æˆæœ¬è¿½è¸ª**
- ç®€å•é—®é¢˜è‡ªåŠ¨è·¯ç”±åˆ°ä¾¿å®œæ¨¡å‹
- å®æ—¶æ˜¾ç¤ºå½“å‰ä¼šè¯æˆæœ¬
- æœˆåº¦/å‘¨åº¦æˆæœ¬æŠ¥å‘Š

### ç—›ç‚¹5: ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ â­
**é—®é¢˜**:
- å¤§å‹ä»£ç åº“è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶
- éœ€è¦ç²¾ç»†æ§åˆ¶å‘é€ç»™ LLM çš„å†…å®¹

**è§£å†³æ–¹æ¡ˆ**: **ä¸Šä¸‹æ–‡ç®¡ç†**
- æ™ºèƒ½è£å‰ªä¸Šä¸‹æ–‡
- ä¼˜å…ˆä¿ç•™é‡è¦ä¿¡æ¯
- å¯é…ç½®ä¸Šä¸‹æ–‡ç­–ç•¥

---

## åŠŸèƒ½éœ€æ±‚æ¸…å•

### 1. æ ¸å¿ƒåŠŸèƒ½ï¼ˆMust Haveï¼‰

#### 1.1 åŒå‘åè®®ç¿»è¯‘ â­â­â­
```yaml
translation:
  enabled: true
  bidirectional: true

  # OpenAI â†’ Anthropic
  openai_to_anthropic:
    enabled: true
    preserve_tool_calls: true
    preserve_streaming: true

  # Anthropic â†’ OpenAI
  anthropic_to_openai:
    enabled: true
    preserve_tool_use: true
    preserve_streaming: true

  # è‡ªåŠ¨æ£€æµ‹å’Œé€‰æ‹©
  auto_detect: true
```

**å®ç°ç»†èŠ‚**:
```python
class ProtocolTranslator:
    def translate_openai_to_anthropic(self, request):
        """
        OpenAI Chat Completions â†’ Anthropic Messages

        æ˜ å°„:
        - messages[] â†’ messages[]
        - functions[] â†’ tools[]
        - function_call â†’ tool_choice
        - stream â†’ stream
        """
        pass

    def translate_anthropic_to_openai(self, request):
        """
        Anthropic Messages â†’ OpenAI Chat Completions

        æ˜ å°„:
        - messages[] â†’ messages[]
        - tools[] â†’ functions[]
        - tool_choice â†’ function_call
        - stream â†’ stream
        """
        pass
```

#### 1.2 å·¥å…·è°ƒç”¨å»é‡ â­â­â­
```yaml
tool_calling:
  enabled: true

  # å»é‡é…ç½®
  deduplication:
    enabled: true
    strategy: exact_match  # exact_match | semantic_similarity

    # æ£€æµ‹çª—å£
    window_size: 10  # æ£€æŸ¥æœ€è¿‘10æ¬¡è°ƒç”¨

    # ç›¸åŒè°ƒç”¨å®šä¹‰
    exact_match:
      compare_name: true
      compare_arguments: true
      ignore_fields: [timestamp, id]

    # è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¯é€‰ï¼‰
    semantic_similarity:
      threshold: 0.95
      model: sentence-transformers

  # æ— é™å¾ªç¯æ£€æµ‹
  loop_detection:
    enabled: true
    max_same_tool_calls: 3  # åŒä¸€å·¥å…·æœ€å¤šè°ƒç”¨3æ¬¡
    action: warn_and_break  # warn_and_break | warn | break

  # è°ƒè¯•æ¨¡å¼
  debug:
    log_all_tool_calls: true
    alert_on_duplicate: true
```

**å®ç°ç»†èŠ‚**:
```python
class ToolCallDeduplicator:
    def __init__(self, window_size=10):
        self.recent_calls = []
        self.window_size = window_size

    def is_duplicate(self, tool_call):
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        for recent in self.recent_calls[-self.window_size:]:
            if self._is_same_call(tool_call, recent):
                return True
        return False

    def _is_same_call(self, call1, call2):
        """æ¯”è¾ƒä¸¤ä¸ªtool callæ˜¯å¦ç›¸åŒ"""
        return (
            call1['name'] == call2['name'] and
            call1['arguments'] == call2['arguments']
        )

    def detect_loop(self):
        """æ£€æµ‹æ— é™å¾ªç¯"""
        # ç»Ÿè®¡æœ€è¿‘Næ¬¡è°ƒç”¨ä¸­ï¼Œæ¯ä¸ªå·¥å…·è¢«è°ƒç”¨çš„æ¬¡æ•°
        tool_counts = {}
        for call in self.recent_calls[-self.window_size:]:
            name = call['name']
            tool_counts[name] = tool_counts.get(name, 0) + 1

        # å¦‚æœæŸä¸ªå·¥å…·è¢«è°ƒç”¨è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯å¾ªç¯
        for name, count in tool_counts.items():
            if count > self.max_same_tool_calls:
                return True, name
        return False, None
```

#### 1.3 é•¿è¶…æ—¶æ”¯æŒ â­â­
```yaml
timeout:
  # Coding åœºæ™¯éœ€è¦æ›´é•¿è¶…æ—¶
  request_timeout: 300  # 5åˆ†é’Ÿ
  streaming_timeout: 600  # 10åˆ†é’Ÿï¼ˆstreamingå¯ä»¥æ›´é•¿ï¼‰

  # ä¿æ´»æœºåˆ¶
  keepalive:
    enabled: true
    interval: 30  # æ¯30ç§’å‘é€heartbeat
```

#### 1.4 æˆæœ¬è¿½è¸ª â­â­
```yaml
cost_tracking:
  enabled: true

  # å®æ—¶æ˜¾ç¤º
  real_time:
    enabled: true
    show_per_request: true
    show_session_total: true

  # æˆæœ¬æŠ¥å‘Š
  reporting:
    daily_summary: true
    weekly_summary: true
    monthly_summary: true
    email_report: false

  # é¢„ç®—å‘Šè­¦
  budget_alerts:
    enabled: true
    daily_limit: 10  # $10/å¤©
    monthly_limit: 300  # $300/æœˆ
    alert_at_percentage: 80  # 80%æ—¶å‘Šè­¦
```

**å®ç°ç»†èŠ‚**:
```python
class CostTracker:
    def calculate_cost(self, model, input_tokens, output_tokens):
        """è®¡ç®—å•æ¬¡è¯·æ±‚æˆæœ¬"""
        pricing = {
            'gpt-4-turbo': {
                'input': 0.01 / 1000,   # $0.01 per 1K tokens
                'output': 0.03 / 1000,
            },
            'gpt-3.5-turbo': {
                'input': 0.0015 / 1000,
                'output': 0.002 / 1000,
            },
            'claude-3-opus': {
                'input': 0.015 / 1000,
                'output': 0.075 / 1000,
            },
            'claude-3-sonnet': {
                'input': 0.003 / 1000,
                'output': 0.015 / 1000,
            },
        }

        model_pricing = pricing.get(model, pricing['gpt-3.5-turbo'])
        cost = (
            input_tokens * model_pricing['input'] +
            output_tokens * model_pricing['output']
        )
        return cost

    def get_session_cost(self, session_id):
        """è·å–ä¼šè¯æ€»æˆæœ¬"""
        pass

    def check_budget(self, user_id):
        """æ£€æŸ¥æ˜¯å¦è¶…é¢„ç®—"""
        daily_usage = self.get_daily_usage(user_id)
        if daily_usage > self.daily_limit:
            raise BudgetExceededError(f"Daily limit ${self.daily_limit} exceeded")
```

---

### 2. é«˜çº§åŠŸèƒ½ï¼ˆShould Haveï¼‰

#### 2.1 æ™ºèƒ½è·¯ç”± â­â­
```yaml
intelligent_routing:
  enabled: true

  # åŸºäºå¤æ‚åº¦è·¯ç”±
  complexity_based:
    enabled: true

    # ç®€å•ä»»åŠ¡ï¼ˆcheap modelï¼‰
    simple_tasks:
      model: gpt-3.5-turbo
      triggers:
        - short_prompt  # < 100 tokens
        - simple_keywords: ["explain", "what is", "how to"]

    # å¤æ‚ä»»åŠ¡ï¼ˆexpensive modelï¼‰
    complex_tasks:
      model: gpt-4-turbo
      triggers:
        - long_prompt  # > 500 tokens
        - complex_keywords: ["refactor", "optimize", "debug"]
        - has_code_blocks: true

  # åŸºäºæˆæœ¬è·¯ç”±
  cost_based:
    enabled: true
    prefer_cheaper: true
    fallback_to_expensive: true  # ä¾¿å®œæ¨¡å‹å¤±è´¥æ—¶åˆ‡æ¢

  # è´Ÿè½½å‡è¡¡
  load_balancing:
    enabled: true
    strategy: round_robin  # round_robin | least_latency | random
```

#### 2.2 ä¸Šä¸‹æ–‡ç®¡ç† â­
```yaml
context_management:
  enabled: true

  # ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
  max_context_tokens: 8000

  # è£å‰ªç­–ç•¥
  truncation_strategy: smart  # smart | head | tail | middle

  # æ™ºèƒ½è£å‰ª
  smart_truncation:
    # ä¼˜å…ˆçº§
    priority:
      - system_message  # ç³»ç»Ÿæ¶ˆæ¯ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
      - recent_messages  # æœ€è¿‘çš„å¯¹è¯
      - code_blocks  # ä»£ç å—
      - tool_results  # å·¥å…·è°ƒç”¨ç»“æœ
      - older_messages  # æ—§å¯¹è¯ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰

    # ä¿ç•™ç­–ç•¥
    preserve:
      min_recent_messages: 5  # è‡³å°‘ä¿ç•™æœ€è¿‘5æ¡
      min_system_tokens: 200  # ç³»ç»Ÿæ¶ˆæ¯è‡³å°‘200 tokens
```

#### 2.3 ç¼“å­˜ä¼˜åŒ– â­
```yaml
caching:
  enabled: true

  # ä»£ç ç›¸å…³çš„æŸ¥è¯¢å¾ˆé€‚åˆç¼“å­˜
  cache_candidates:
    - documentation_queries  # "what is X?"
    - error_explanations     # "what does this error mean?"
    - simple_code_snippets   # "write a function to..."

  # ç¼“å­˜é…ç½®
  ttl: 3600  # 1å°æ—¶
  max_cache_size: 1000  # æœ€å¤š1000æ¡

  # è¯­ä¹‰ç¼“å­˜ï¼ˆé«˜çº§ï¼‰
  semantic_cache:
    enabled: false  # å¯é€‰
    similarity_threshold: 0.95
```

#### 2.4 å“åº”ä¼˜åŒ– â­
```yaml
response_optimization:
  # æµå¼å“åº”ä¼˜åŒ–
  streaming:
    enabled: true
    chunk_size: 50  # æ¯50ä¸ªtokenå‘é€ä¸€æ¬¡

  # ä»£ç æ ¼å¼åŒ–
  code_formatting:
    enabled: true
    auto_detect_language: true
    syntax_highlighting: false  # IDE è‡ªå·±ä¼šåš

  # Markdown æ¸²æŸ“
  markdown:
    enabled: true
    preserve_code_blocks: true
```

---

### 3. è°ƒè¯•å’Œç›‘æ§ï¼ˆNice to Haveï¼‰

#### 3.1 è¯¦ç»†æ—¥å¿— â­â­
```yaml
logging:
  level: info  # debug | info | warn | error

  # Coder ç‰¹å®šæ—¥å¿—
  coder_logs:
    log_protocol_translation: true
    log_tool_calls: true
    log_cost_per_request: true
    log_latency: true

  # æ–‡ä»¶è¾“å‡º
  file:
    enabled: true
    path: ~/.tokligence/coder.log
    rotate: daily
```

#### 3.2 æ€§èƒ½ç›‘æ§
```yaml
monitoring:
  enabled: true

  # Coder å…³å¿ƒçš„æŒ‡æ ‡
  metrics:
    - request_latency
    - tokens_per_request
    - cost_per_request
    - tool_call_count
    - duplicate_tool_calls
    - cache_hit_rate

  # Dashboard
  dashboard:
    enabled: false  # CLI ç”¨æˆ·å¯èƒ½ä¸éœ€è¦
    port: 9090
```

#### 3.3 è°ƒè¯•æ¨¡å¼
```yaml
debug:
  enabled: false

  # è¯¦ç»†ä¿¡æ¯
  verbose:
    show_raw_requests: false
    show_raw_responses: false
    show_translations: true  # æ˜¾ç¤ºåè®®ç¿»è¯‘è¿‡ç¨‹

  # Dry run
  dry_run:
    enabled: false
    skip_actual_api_calls: true
```

---

## é…ç½®æ–‡ä»¶æ¨¡æ¿

### coder.yaml (å®Œæ•´é…ç½®)

```yaml
# Tokligence Gateway - Coder Profile
# ä¼˜åŒ–ç”¨äº Claude Code, Cursor, Codex CLI ç­‰ coding agent

profile: coder
version: 0.3.4

gateway:
  port: 8081
  work_mode: auto  # æ™ºèƒ½è·¯ç”±å’Œç¿»è¯‘

  # æœ¬åœ°ä½¿ç”¨ä¸éœ€è¦è®¤è¯
  auth:
    enabled: false

  # Coding éœ€è¦æ›´é•¿è¶…æ—¶
  timeout:
    request: 300  # 5åˆ†é’Ÿ
    streaming: 600  # 10åˆ†é’Ÿ

  # è¾ƒé«˜å¹¶å‘ï¼ˆIDE å¯èƒ½åŒæ—¶å‘å¤šä¸ªè¯·æ±‚ï¼‰
  max_concurrent_requests: 20

database:
  # ä¸ªäººä½¿ç”¨ SQLite è¶³å¤Ÿ
  type: sqlite
  path: ~/.tokligence/coder.db

providers:
  # OpenAI
  openai:
    enabled: true
    api_key: ${OPENAI_API_KEY}
    models:
      - gpt-4-turbo          # å¤æ‚ä»»åŠ¡
      - gpt-4                # å¤‡é€‰
      - gpt-3.5-turbo        # ç®€å•ä»»åŠ¡ï¼ˆçœé’±ï¼‰

  # Anthropic
  anthropic:
    enabled: true
    api_key: ${ANTHROPIC_API_KEY}
    models:
      - claude-3-opus-20240229      # æœ€å¼ºï¼Œå¤æ‚ä»£ç 
      - claude-3-sonnet-20240229    # å¹³è¡¡æ€§ä»·æ¯”
      - claude-3-haiku-20240307     # å¿«é€Ÿç®€å•ä»»åŠ¡

  # Google (å¯é€‰)
  google:
    enabled: false
    api_key: ${GOOGLE_API_KEY}

  # Local LLM (å¯é€‰)
  ollama:
    enabled: false
    base_url: http://localhost:11434

# æ ¸å¿ƒåŠŸèƒ½ï¼šåè®®ç¿»è¯‘
translation:
  enabled: true
  bidirectional: true
  preserve_tool_calls: true
  preserve_streaming: true
  auto_detect: true

# æ ¸å¿ƒåŠŸèƒ½ï¼šå·¥å…·è°ƒç”¨å»é‡
tool_calling:
  enabled: true

  deduplication:
    enabled: true
    strategy: exact_match
    window_size: 10

  loop_detection:
    enabled: true
    max_same_tool_calls: 3
    action: warn_and_break

  debug:
    log_all_tool_calls: true
    alert_on_duplicate: true

# æˆæœ¬è¿½è¸ª
cost_tracking:
  enabled: true

  real_time:
    enabled: true
    show_per_request: true
    show_session_total: true

  reporting:
    daily_summary: true
    weekly_summary: false
    monthly_summary: true

  budget_alerts:
    enabled: true
    daily_limit: 10      # $10/å¤©
    monthly_limit: 300   # $300/æœˆ
    alert_at_percentage: 80

# æ™ºèƒ½è·¯ç”±ï¼ˆå¯é€‰ï¼‰
intelligent_routing:
  enabled: true

  complexity_based:
    enabled: true
    simple_tasks:
      model: gpt-3.5-turbo
      triggers:
        - short_prompt
        - simple_keywords: ["explain", "what is", "define"]
    complex_tasks:
      model: gpt-4-turbo
      triggers:
        - long_prompt
        - complex_keywords: ["refactor", "optimize", "architect"]
        - has_code_blocks: true

  cost_based:
    enabled: true
    prefer_cheaper: true

# ä¸Šä¸‹æ–‡ç®¡ç†
context_management:
  enabled: true
  max_context_tokens: 8000
  truncation_strategy: smart

  smart_truncation:
    priority:
      - system_message
      - recent_messages
      - code_blocks
      - tool_results
    preserve:
      min_recent_messages: 5

# ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
caching:
  enabled: true
  ttl: 3600
  cache_candidates:
    - documentation_queries
    - error_explanations
    - simple_code_snippets

# æ—¥å¿—
logging:
  level: info
  coder_logs:
    log_protocol_translation: true
    log_tool_calls: true
    log_cost_per_request: true
  file:
    enabled: true
    path: ~/.tokligence/coder.log

# ç›‘æ§
monitoring:
  enabled: true
  metrics:
    - request_latency
    - tokens_per_request
    - cost_per_request
    - tool_call_count
    - duplicate_tool_calls
```

---

## ä½¿ç”¨åœºæ™¯ç¤ºä¾‹

### åœºæ™¯1: Claude Code â†’ OpenAI GPT-4

```bash
# 1. å®‰è£…
pip install tokligence

# 2. åˆå§‹åŒ– coder profile
tokligence init --profile coder

# 3. é…ç½® API keys
export OPENAI_API_KEY=sk-...

# 4. å¯åŠ¨ gateway
tokligence-daemon start --background

# 5. é…ç½® Claude Code
# åœ¨ Claude Code è®¾ç½®ä¸­ï¼š
# Base URL: http://localhost:8081/anthropic
# API Key: (å¯ä»¥ç•™ç©ºæˆ–éšæ„å¡«)

# 6. åœ¨ Claude Code ä¸­é€‰æ‹©æ¨¡å‹
# é€‰æ‹© "gpt-4-turbo" æˆ– "gpt-3.5-turbo"
# Gateway ä¼šè‡ªåŠ¨ç¿»è¯‘ Anthropic API åˆ° OpenAI API
```

**å·¥ä½œæµç¨‹**:
```
Claude Code (Anthropic SDK)
    â†“
    å‘é€ Anthropic Messages API è¯·æ±‚
    POST http://localhost:8081/anthropic/v1/messages
    {
      "model": "gpt-4-turbo",  # å®é™…æ˜¯ OpenAI æ¨¡å‹
      "messages": [...],
      "tools": [...]
    }
    â†“
Tokligence Gateway
    â†“
    æ£€æµ‹åˆ° model="gpt-4-turbo" â†’ OpenAI
    â†“
    ç¿»è¯‘ Anthropic â†’ OpenAI
    {
      "model": "gpt-4-turbo",
      "messages": [...],
      "functions": [...]  # tools â†’ functions
    }
    â†“
    è°ƒç”¨ OpenAI API
    â†“
    ç¿»è¯‘å“åº” OpenAI â†’ Anthropic
    â†“
Claude Code æ”¶åˆ° Anthropic æ ¼å¼å“åº”
```

### åœºæ™¯2: Cursor â†’ Anthropic Claude

```bash
# é…ç½® Cursor
# Settings â†’ Models â†’ Custom OpenAI API
# Base URL: http://localhost:8081/v1
# API Key: (éšæ„å¡«)

# åœ¨ Cursor ä¸­é€‰æ‹©æ¨¡å‹: claude-3-sonnet-20240229
# Gateway è‡ªåŠ¨ç¿»è¯‘ OpenAI â†’ Anthropic
```

### åœºæ™¯3: æˆæœ¬ä¼˜åŒ– - æ™ºèƒ½è·¯ç”±

```python
# é…ç½®æ™ºèƒ½è·¯ç”±åï¼ŒGateway ä¼šè‡ªåŠ¨é€‰æ‹©æ¨¡å‹

# ç®€å•é—®é¢˜ â†’ è‡ªåŠ¨ç”¨ gpt-3.5-turbo
"What is a Python decorator?"

# å¤æ‚é—®é¢˜ â†’ è‡ªåŠ¨ç”¨ gpt-4-turbo
"Refactor this 500-line class to follow SOLID principles"

# æŸ¥çœ‹æˆæœ¬
$ tokligence usage --today
Today's usage: $2.34
  - gpt-3.5-turbo: $0.12 (120 requests)
  - gpt-4-turbo: $2.22 (8 requests)
```

---

## CLI å‘½ä»¤æ‰©å±•

### ä¸“é—¨ä¸º Coder Profile çš„å‘½ä»¤

```bash
# æŸ¥çœ‹å®æ—¶æˆæœ¬
tokligence coder cost --live

# æŸ¥çœ‹å·¥å…·è°ƒç”¨ç»Ÿè®¡
tokligence coder tools --stats

# æ£€æµ‹åˆ°çš„é‡å¤è°ƒç”¨
tokligence coder duplicates --last 100

# æ€§èƒ½ç»Ÿè®¡
tokligence coder perf
# Output:
# Average latency: 1.2s
# P95 latency: 3.5s
# Cache hit rate: 45%
# Cost per request: $0.02

# å¯¼å‡º coding session
tokligence coder export --session <id> --format json
```

---

## Python API æ‰©å±•

```python
from tokligence import Gateway
from tokligence.coder import CoderSession, CostTracker, ToolCallMonitor

# åˆå§‹åŒ– Coder ä¼˜åŒ–çš„ Gateway
gateway = Gateway.from_profile('coder')

# åˆ›å»º coding session
session = CoderSession(gateway)

# å®æ—¶æˆæœ¬è¿½è¸ª
cost_tracker = CostTracker(session)
print(f"Current session cost: ${cost_tracker.get_current_cost()}")

# å·¥å…·è°ƒç”¨ç›‘æ§
tool_monitor = ToolCallMonitor(session)
if tool_monitor.has_duplicates():
    print("Warning: Duplicate tool calls detected!")
    tool_monitor.show_duplicates()

# æ™ºèƒ½è·¯ç”±é…ç½®
from tokligence.coder import IntelligentRouter

router = IntelligentRouter(gateway)
router.set_simple_model('gpt-3.5-turbo')
router.set_complex_model('gpt-4-turbo')
router.enable_cost_based_routing()
```

---

## å®ç°ä¼˜å…ˆçº§

åŸºäºé‡è¦æ€§å’Œéš¾åº¦ï¼Œå»ºè®®å®ç°é¡ºåºï¼š

| åŠŸèƒ½ | ä¼˜å…ˆçº§ | éš¾åº¦ | é¢„ä¼°æ—¶é—´ |
|------|--------|------|---------|
| åŒå‘åè®®ç¿»è¯‘ | P0 | é«˜ | 2-3å¤© |
| å·¥å…·è°ƒç”¨å»é‡ | P0 | ä¸­ | 1-2å¤© |
| é•¿è¶…æ—¶æ”¯æŒ | P0 | ä½ | 0.5å¤© |
| æˆæœ¬è¿½è¸ª | P1 | ä¸­ | 1-2å¤© |
| æ™ºèƒ½è·¯ç”± | P1 | ä¸­ | 2å¤© |
| ä¸Šä¸‹æ–‡ç®¡ç† | P2 | ä¸­ | 1-2å¤© |
| ç¼“å­˜ä¼˜åŒ– | P2 | ä½ | 1å¤© |
| CLI æ‰©å±• | P2 | ä½ | 1å¤© |

**MVP (Minimum Viable Product)**:
- Phase 1: åŒå‘ç¿»è¯‘ + å»é‡ + é•¿è¶…æ—¶ (æ ¸å¿ƒåŠŸèƒ½)
- Phase 2: æˆæœ¬è¿½è¸ª + æ™ºèƒ½è·¯ç”± (å¢å€¼åŠŸèƒ½)
- Phase 3: å…¶ä»–ä¼˜åŒ–åŠŸèƒ½

---

## æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•
```python
# test_protocol_translation.py
def test_openai_to_anthropic_translation():
    """æµ‹è¯• OpenAI â†’ Anthropic ç¿»è¯‘"""
    pass

def test_anthropic_to_openai_translation():
    """æµ‹è¯• Anthropic â†’ OpenAI ç¿»è¯‘"""
    pass

# test_tool_deduplication.py
def test_exact_match_deduplication():
    """æµ‹è¯•ç²¾ç¡®åŒ¹é…å»é‡"""
    pass

def test_loop_detection():
    """æµ‹è¯•æ— é™å¾ªç¯æ£€æµ‹"""
    pass

# test_cost_tracking.py
def test_cost_calculation():
    """æµ‹è¯•æˆæœ¬è®¡ç®—"""
    pass
```

### é›†æˆæµ‹è¯•
```python
# test_coder_integration.py
def test_claude_code_to_openai():
    """æµ‹è¯• Claude Code â†’ OpenAI å®Œæ•´æµç¨‹"""
    # 1. å¯åŠ¨ gateway
    # 2. æ¨¡æ‹Ÿ Claude Code è¯·æ±‚
    # 3. éªŒè¯ç¿»è¯‘æ­£ç¡®
    # 4. éªŒè¯å“åº”æ ¼å¼
    pass

def test_cursor_to_anthropic():
    """æµ‹è¯• Cursor â†’ Anthropic å®Œæ•´æµç¨‹"""
    pass

def test_tool_call_deduplication_e2e():
    """æµ‹è¯•å·¥å…·è°ƒç”¨å»é‡ç«¯åˆ°ç«¯"""
    # æ¨¡æ‹Ÿé‡å¤çš„ tool calls
    # éªŒè¯è¢«æ­£ç¡®å»é‡
    pass
```

### æ€§èƒ½æµ‹è¯•
```python
def test_translation_latency():
    """æµ‹è¯•ç¿»è¯‘å»¶è¿Ÿ < 10ms"""
    pass

def test_high_concurrency():
    """æµ‹è¯•é«˜å¹¶å‘åœºæ™¯ï¼ˆ20 concurrent requestsï¼‰"""
    pass
```

---

## æ–‡æ¡£éœ€æ±‚

1. **Quick Start Guide** - Coder Profile 5åˆ†é’Ÿä¸Šæ‰‹
2. **Protocol Translation Guide** - åè®®ç¿»è¯‘è¯¦è§£
3. **Cost Optimization Guide** - æˆæœ¬ä¼˜åŒ–æœ€ä½³å®è·µ
4. **Troubleshooting** - å¸¸è§é—®é¢˜ï¼ˆå»é‡ä¸å·¥ä½œã€ç¿»è¯‘é”™è¯¯ç­‰ï¼‰
5. **API Reference** - Python API æ–‡æ¡£

---

## æ€»ç»“

Coder Profile çš„æ ¸å¿ƒä»·å€¼ï¼š

1. **ğŸ”„ åè®®è‡ªç”±** - ä¸å†è¢«å·¥å…·é”å®šåè®®ï¼Œæƒ³ç”¨å“ªä¸ªæ¨¡å‹ç”¨å“ªä¸ª
2. **ğŸ›¡ï¸ é˜²æ— é™å¾ªç¯** - æ™ºèƒ½å»é‡ï¼ŒèŠ‚çœtokenå’Œæˆæœ¬
3. **ğŸ’° æˆæœ¬å¯æ§** - å®æ—¶è¿½è¸ªï¼Œæ™ºèƒ½è·¯ç”±ï¼Œé¢„ç®—å‘Šè­¦
4. **âš¡ æ€§èƒ½ä¼˜åŒ–** - é•¿è¶…æ—¶ã€ç¼“å­˜ã€ä¸Šä¸‹æ–‡ç®¡ç†

ä¸‹ä¸€æ­¥ï¼š
- ç¡®è®¤è¿™äº›åŠŸèƒ½æ˜¯å¦ç¬¦åˆä½ çš„é¢„æœŸ
- å†³å®š MVP åŒ…å«å“ªäº›åŠŸèƒ½
- å¼€å§‹å®ç° Phase 1 æ ¸å¿ƒåŠŸèƒ½
